# ML Service Configuration
# Copy to .env and update values for your environment

# Model Configuration
# Path to trained model directory containing config.json and model weights
MODEL_PATH=/models/best_model

# Model version for tracking and logging
MODEL_VERSION=v1.0.0

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Device for inference: 'cpu' or 'cuda'
# Use 'cuda' if GPU is available for faster inference
DEVICE=cpu

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Redis Configuration (for caching)
REDIS_URL=redis://redis:6379
CACHE_TTL=3600  # Cache time-to-live in seconds


# ============================================
# backend/.env.example
# ============================================
# Backend API Configuration

# Environment: development, staging, production
NODE_ENV=development

# Server Configuration
PORT=5000
HOST=0.0.0.0

# Database Configuration
# MongoDB connection string
# Format: mongodb://username:password@host:port/database?options
MONGODB_URI=mongodb://admin:password@mongodb:27017/prompt_optimizer?authSource=admin

# ML Service Configuration
# Internal Docker network URL (service name)
ML_SERVICE_URL=http://ml-service:8000

# Redis Configuration
REDIS_URL=redis://redis:6379

# CORS Configuration
# Comma-separated list of allowed origins
CORS_ORIGIN=http://localhost:3000,http://localhost:5173

# Rate Limiting
# Maximum requests per window
RATE_LIMIT_MAX=10
# Window duration in milliseconds (60000ms = 1 minute)
RATE_LIMIT_WINDOW_MS=60000

# JWT Configuration (for authentication - Phase 2)
JWT_SECRET=your-super-secret-jwt-key-change-in-production
JWT_EXPIRES_IN=7d

# Logging
LOG_LEVEL=info
# Log file paths
LOG_ERROR_FILE=logs/error.log
LOG_COMBINED_FILE=logs/combined.log


# ============================================
# frontend/.env.example
# ============================================
# Frontend Configuration

# API Base URL
# Points to backend API endpoint
VITE_API_URL=http://localhost:5000/api/v1

# App Configuration
VITE_APP_NAME="Prompt Optimizer"
VITE_APP_VERSION=1.0.0

# Feature Flags
VITE_ENABLE_ANALYTICS=false
VITE_ENABLE_AUTH=false

# Environment
VITE_ENV=development


# ============================================
# ml-model-training/.env.example
# ============================================
# ML Training Configuration

# Weights & Biases (Experiment Tracking)
WANDB_API_KEY=your-wandb-api-key
WANDB_PROJECT=prompt-optimizer
WANDB_ENTITY=your-username

# Training Configuration
BATCH_SIZE=8
LEARNING_RATE=5e-5
NUM_EPOCHS=10
GRADIENT_ACCUMULATION_STEPS=4

# Model Configuration
MODEL_NAME=google/flan-t5-base
MAX_INPUT_LENGTH=256
MAX_OUTPUT_LENGTH=512

# Data Paths
DATA_DIR=./data
MODEL_DIR=./models
OUTPUT_DIR=./outputs

# Compute
DEVICE=cuda  # or 'cpu'
MIXED_PRECISION=true


# ============================================
# PRODUCTION NOTES
# ============================================

# For production deployment:
# 1. Generate strong random secrets:
#    openssl rand -base64 32
#
# 2. Use environment-specific values:
#    - Production MongoDB with authentication
#    - Production Redis with password
#    - Secure JWT secrets
#    - Enable HTTPS/SSL
#
# 3. Never commit .env files to Git
#    Always use .env.example as template
#
# 4. Use secret management services:
#    - AWS Secrets Manager
#    - Azure Key Vault
#    - Google Secret Manager
#    - HashiCorp Vault
#
# 5. Set appropriate rate limits for production
#    - Higher limits for authenticated users
#    - Lower limits for anonymous users